# -*- coding: utf-8 -*-
"""Sentimental Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WG8YcgHCU2b6MQQK91UMTxN5__0Um1zS
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from sklearn.feature_extraction.text import CountVectorizer

df= pd.read_csv('disaster_tweets_data(DS).csv')
df.head()

df.isnull() #checking null values

df.dtypes #checking types

df.duplicated() # checking duplicacy

# Preprocessing the text data
def preprocess_text(text):
    text = text.lower() # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text) # Remove punctuation
    words = text.split()  # Tokenize words
    # Remove stop words (you can use NLTK or a predefined list)
    stop_words = set(['the', 'and', 'is', 'in', 'to', 'of'])  # Add more as needed
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

# Apply preprocessing to the 'tweets' column
df['cleaned_tweets'] = df['tweets'].apply(preprocess_text)
df.head()

df.drop(columns=['tweets'],inplace = True)
df.head()

#vectorize the text data
cv=CountVectorizer()
x= cv.fit_transform(df['cleaned_tweets'])
y= df['target']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.30, random_state= 42)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#Implementing Naive Bayes
from sklearn.naive_bayes import MultinomialNB
m1= MultinomialNB()
m1.fit(x_train, y_train)

#Accuracy
print('Train score', m1.score(x_train,y_train))
print('Test score', m1.score(x_test,y_test))

ypred_m1 = m1.predict(x_test)
print(ypred_m1)

from sklearn.metrics import confusion_matrix, classification_report

print(confusion_matrix(y_test,ypred_m1)) #ypred_m1 instead of y_pred
print(classification_report(y_test,ypred_m1))
#caluclating the accuracy
accuracy_model1 = m1.score(x_test,y_test)
accuracy_model1

#Implementing Logistic Regression
from sklearn.linear_model import LogisticRegression
m2= LogisticRegression(solver="liblinear")
m2.fit(x_train,y_train)
#accuracy
print('Train_score',m2.score(x_train,y_train))
print('Test_score',m2.score(x_test,y_test))

ypred_m2= m2.predict(x_test)
print(ypred_m2)

from sklearn.metrics import confusion_matrix, classification_report
confusion_m2 = confusion_matrix(y_test,ypred_m2)
print(confusion_m2)
print(classification_report(y_test,ypred_m2))
#caluclating the accuracy
accuracy_model2 = m2.score(x_test,y_test)
accuracy_model2

# Implementing KNN
from sklearn.neighbors import KNeighborsClassifier

m3 = KNeighborsClassifier(n_neighbors=85)
m3.fit(x_train,y_train)
#accuracy
print('Train_score',m3.score(x_train,y_train))
print('Test_score',m3.score(x_test,y_test))

ypred_m3 = m3.predict(x_test)
print(ypred_m3)

confusion_m3 = confusion_matrix(y_test,ypred_m3)
print(confusion_m3)
print(classification_report(y_test,ypred_m3))
#caluclating the accuracy
accuracy_model3 = m3.score(x_test,y_test)
accuracy_model3

# Reporting the best acurracy
best_model = max(accuracy_model1,accuracy_model2,accuracy_model3)
if (best_model == accuracy_model1):
    print("MNB Classification has the best accuracy with the given dataset", m1.score(x_test,y_test) * 100, "%")
elif (best_model == accuracy_model2):
    print("Logistic Regression has the best accuracy with the given dataset", m2.score(x_test,y_test) * 100, "%")
elif (best_model == accuracy_model3):
    print("KNN Classifer has the best accuracy with the given dataset", m3.score(x_test,y_test) * 100, "%")



